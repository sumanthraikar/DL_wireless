{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '/home/sumanthraikar/Desktop/Casual Learning/pyadi_iio')\n",
    "import adi \n",
    "from utils import Tx_setting, Rx_setting\n",
    "import matplotlib.pyplot as plt\n",
    "from fec_blocks import FECHamming\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from model import NN_Symbol_demapper, NN_Symbol_Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimum hamming distance loss function\n",
    "class Min_hamming_distance(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        super(Min_hamming_distance, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, encoder):\n",
    "        codebook = encoder(torch.eye(2**self.k))\n",
    "        return -self.alpha*torch.cdist(codebook, codebook).flatten().kthvalue((2**self.k)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Communication_system_training:\n",
    "\n",
    "    def __init__(self, n,k,act_fn, batch_size,sps=20, lr=1e-3):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.act_fn = act_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.sps = sps\n",
    "        self.lr = lr\n",
    "\n",
    "        #----------------------Tx_definition------------------------- \n",
    "        self.sample_rate = 1e6\n",
    "        self.center_freq = 915e6 \n",
    "\n",
    "        self.encoder = NN_Symbol_Mapper(act_fn=self.act_fn, n=self.n, k = self.k)\n",
    "        self.encoder.mapper.load_state_dict(torch.load('alternate_AE_encoder_model_k4N7.pth'))\n",
    "\n",
    "        self.pluto_tx = Tx_setting(sample_rate=self.sample_rate,center_freq=self.center_freq)\n",
    "        self.E_optimizer = torch.optim.Adam(\n",
    "        self.encoder.parameters(), lr=self.lr)\n",
    "\n",
    "        #----------------------Rx definition-----------------------------\n",
    "        self.pluto_rx = Rx_setting(sample_rate=self.sample_rate,center_freq=self.center_freq,data_length=self.n, sps=self.sps)\n",
    "        self.demapper = NN_Symbol_demapper(act_fn=self.act_fn, n=self.n, k=self.k)\n",
    "        self.demapper.s2b_mapper.load_state_dict(torch.load('alterante_AE_decoder_model_k4N7.pth'))\n",
    "        self.E_loss = nn.CrossEntropyLoss()\n",
    "        self.D_loss = nn.CrossEntropyLoss()\n",
    "        self.D_optimizer = torch.optim.Adam(\n",
    "        self.demapper.parameters(), lr=self.lr)\n",
    "\n",
    "    def encoder_loss(self):\n",
    "        return torch.cdist(self.encoder(torch.eye(2**self.k)), self.encoder(torch.eye(2**self.k))).mean()\n",
    "\n",
    "    \n",
    "\n",
    "    def one_hot_converter(self,array):\n",
    "    #----Accepts a matrix in binary format and converts to one_hot tensor form---\n",
    "        batch_size, code_length = array.shape\n",
    "        one_hot = np.zeros((batch_size, 2**code_length))\n",
    "        if (array!=None).any():\n",
    "            b = [''.join([str(int(j)) for j in i]) for i in array]\n",
    "            c = [int(i,2) for i in b]\n",
    "            one_hot[np.arange(batch_size), c] = 1\n",
    "\n",
    "            return torch.from_numpy(one_hot.astype('float32'))\n",
    "\n",
    "        else:\n",
    "            return torch.from_numpy(one_hot.astype('float32'))\n",
    "        \n",
    "\n",
    "    def data_generate(self):\n",
    "        #-----------------------Data generation-----------------------\n",
    "        data_bits = np.zeros((self.batch_size, self.k))\n",
    "        data_ints = np.random.randint(low=0, high=2**self.k, size=self.batch_size)\n",
    "\n",
    "        for data,i in zip(data_ints,range(self.batch_size)):\n",
    "            data_bits[i] = np.array([int(j) for j in np.binary_repr(data, self.k)])\n",
    "\n",
    "        kb_one_hot_data = self.one_hot_converter(data_bits)\n",
    "\n",
    "    \n",
    "\n",
    "        return kb_one_hot_data.requires_grad_(True)\n",
    "        \n",
    "\n",
    "    def encoder_training(self, count):\n",
    "\n",
    "        self.encoder.train()\n",
    "        avg_loss=0.0\n",
    "        iters=100\n",
    "        for e in range(iters):\n",
    "            kb_one_hot_data = self.data_generate()\n",
    "            \n",
    "            encoded_data_bits = self.encoder(kb_one_hot_data)\n",
    "            received_data_symbols = torch.zeros((self.batch_size, self.n))\n",
    "\n",
    "            for pt,i in zip(encoded_data_bits.detach().numpy(), range(self.batch_size)):\n",
    "                \n",
    "                self.pluto_tx.Modulation(bits=pt, sps=self.sps, repeat_tx=True, plot=False)\n",
    "\n",
    "                received_symbols=self.pluto_rx.demodulate(plot=False)\n",
    "                received_data_symbols[i] = torch.from_numpy(received_symbols.astype('float32'))\n",
    "                self.pluto_tx.sdr.tx_destroy_buffer()\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            decoded_data = self.demapper(received_data_symbols)\n",
    "            loss = self.E_loss(decoded_data, torch.argmax(kb_one_hot_data, 1))\n",
    "            self.E_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.E_optimizer.step()\n",
    "            avg_loss+=loss\n",
    "        print('E_Epoch:{}, ELoss:{:.4f}'.format(e+1,float(avg_loss/iters)))\n",
    "        if (avg_loss/iters) < 0.1:\n",
    "            torch.save(self.encoder.mapper.state_dict(), f'alternate_trained_models/pluto_alternate_AE_encoder_model_k{self.k}N{self.n}c{count}.pth')\n",
    "\n",
    "    \n",
    "    def decoder_training(self, count):\n",
    "\n",
    "        self.demapper.train()\n",
    "        avg_loss=0.0\n",
    "        iters=100\n",
    "        for e in range(iters):\n",
    "            kb_one_hot_data = self.data_generate()\n",
    "            # print(kb_one_hot_data)\n",
    "            encoded_data_bits = self.encoder(kb_one_hot_data)\n",
    "            # print(encoded_data_bits)\n",
    "            received_data_symbols = torch.zeros((self.batch_size, self.n))\n",
    "\n",
    "            for pt,i in zip(encoded_data_bits.detach().numpy(), range(self.batch_size)):\n",
    "                \n",
    "                self.pluto_tx.Modulation(bits=pt, sps=self.sps, repeat_tx=True, plot=False)\n",
    "\n",
    "                received_symbols=self.pluto_rx.demodulate(plot=False)\n",
    "                received_data_symbols[i] = torch.from_numpy(received_symbols.astype('float64'))\n",
    "                self.pluto_tx.sdr.tx_destroy_buffer()\n",
    "\n",
    "            \n",
    "            # print(received_data_symbols)\n",
    "            received_data_symbols.requires_grad_(True)\n",
    "            decoded_data = self.demapper(received_data_symbols)\n",
    "            loss = self.D_loss(decoded_data, torch.argmax(kb_one_hot_data, 1))\n",
    "            self.D_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.D_optimizer.step()\n",
    "            avg_loss+=loss\n",
    "\n",
    "        print('DEpoch:{}, DLoss:{:.4f}'.format(e+1,float(avg_loss/iters)))\n",
    "        if (avg_loss/iters) < 0.1:\n",
    "            torch.save(self.demapper.s2b_mapper.state_dict(), f'alternate_trained_models/pluto_alternate_AE_decoder_model_k{self.k}N{self.n}c{count}.pth')\n",
    "\n",
    "    \n",
    "    def trainer(self):\n",
    "\n",
    "        for i in range(120):\n",
    "            if i%2==1:\n",
    "                self.encoder_training(i)\n",
    "            else:\n",
    "                self.decoder_training(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=7\n",
    "k=4\n",
    "act_fn = nn.ReLU\n",
    "batch_size = 100\n",
    "\n",
    "learner = Communication_system_training(n=n, k=k, act_fn=act_fn, batch_size=batch_size)\n",
    "learner.trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train decoder, encoder\n",
    "#trained model in simulation \n",
    "# start real time training with pretrained models\n",
    "\n",
    "# torch.save(learner.encoder.mapper.state_dict(), f'pluto_alternate_AE_encoder_model_k{k}N{n}.pth')\n",
    "# torch.save(learner.demapper.s2b_mapper.state_dict(), f'pluto_alterante_AE_decoder_model_k{k}N{n}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss = np.array([1.3845, 0.6080, 0.3673,0.2761,0.2222, 0.1872,0.1592,0.1386,0.1225,0.1097,0.0974,0.0880,0.0790,0.0672,0.0605,0.0551,0.0490,0.0435,0.0401])\n",
    "e_loss = np.array([0.8672,0.4317,0.3100,0.2433,0.2062,0.1727,0.1486,0.1285,0.1133,0.1046,0.0918, 0.0802,0.0759,0.0717, 0.0542,0.0515,0.0466,0.0424])\n",
    "plt.plot(d_loss,\"m^-\", label=\"Decoder training CE loss\")\n",
    "plt.plot(e_loss, \"gs-\", label = \"Encoder training CE loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Communication_system_training:\n",
    "\n",
    "    def __init__(self, n,k,act_fn, batch_size,sps=20, lr=1e-3):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.act_fn = act_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.sps = sps\n",
    "        self.lr = lr\n",
    "\n",
    "        #----------------------Tx_definition------------------------- \n",
    "        self.sample_rate = 1e6\n",
    "        self.center_freq = 915e6 \n",
    "\n",
    "        self.encoder = NN_Symbol_Mapper(act_fn=self.act_fn, n=self.n, k = self.k)\n",
    "        self.encoder.mapper.load_state_dict(torch.load('alternate_AE_encoder_model_k4N7.pth'))\n",
    "\n",
    "        self.pluto_tx = Tx_setting(sample_rate=self.sample_rate,center_freq=self.center_freq)\n",
    "        self.E_optimizer = torch.optim.Adam(\n",
    "        self.encoder.parameters(), lr=self.lr)\n",
    "\n",
    "        #----------------------Rx definition-----------------------------\n",
    "        self.pluto_rx = Rx_setting(sample_rate=self.sample_rate,center_freq=self.center_freq,data_length=self.n, sps=self.sps)\n",
    "        self.demapper = NN_Symbol_demapper(act_fn=self.act_fn, n=self.n, k=self.k)\n",
    "        self.demapper.s2b_mapper.load_state_dict(torch.load('alterante_AE_decoder_model_k4N7.pth'))\n",
    "        self.E_loss = nn.CrossEntropyLoss()\n",
    "        self.D_loss = nn.CrossEntropyLoss()\n",
    "        self.D_optimizer = torch.optim.Adam(\n",
    "        self.demapper.parameters(), lr=self.lr)\n",
    "    \n",
    "\n",
    "    def one_hot_converter(self,array):\n",
    "    #----Accepts a matrix in binary format and converts to one_hot tensor form---\n",
    "        batch_size, code_length = array.shape\n",
    "        one_hot = np.zeros((batch_size, 2**code_length))\n",
    "        if (array!=None).any():\n",
    "            b = [''.join([str(int(j)) for j in i]) for i in array]\n",
    "            c = [int(i,2) for i in b]\n",
    "            one_hot[np.arange(batch_size), c] = 1\n",
    "\n",
    "            return torch.from_numpy(one_hot.astype('float32'))\n",
    "\n",
    "        else:\n",
    "            return torch.from_numpy(one_hot.astype('float32'))\n",
    "        \n",
    "\n",
    "    def data_generate(self):\n",
    "        #-----------------------Data generation-----------------------\n",
    "        data_bits = np.zeros((self.batch_size, self.k))\n",
    "        data_ints = np.random.randint(low=0, high=2**self.k, size=self.batch_size)\n",
    "\n",
    "        for data,i in zip(data_ints,range(self.batch_size)):\n",
    "            data_bits[i] = np.array([int(j) for j in np.binary_repr(data, self.k)])\n",
    "\n",
    "        kb_one_hot_data = self.one_hot_converter(data_bits)\n",
    "\n",
    "    \n",
    "\n",
    "        return kb_one_hot_data.requires_grad_(True)\n",
    "        \n",
    "\n",
    "    def encoder_training(self, count):\n",
    "\n",
    "        self.encoder.train()\n",
    "        avg_loss=0.0\n",
    "        iters=100\n",
    "        for e in range(iters):\n",
    "            kb_one_hot_data = self.data_generate()\n",
    "            \n",
    "            encoded_data_bits = self.encoder(kb_one_hot_data)\n",
    "            received_data_symbols = torch.zeros((self.batch_size, self.n))\n",
    "\n",
    "            for pt,i in zip(encoded_data_bits.detach().numpy(), range(self.batch_size)):\n",
    "                \n",
    "                self.pluto_tx.Modulation(bits=pt, sps=self.sps, repeat_tx=True, plot=False)\n",
    "\n",
    "                received_symbols=self.pluto_rx.demodulate(plot=False)\n",
    "                received_data_symbols[i] = torch.from_numpy(received_symbols.astype('float32'))\n",
    "                self.pluto_tx.sdr.tx_destroy_buffer()\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            decoded_data = self.demapper(received_data_symbols)\n",
    "            loss = self.E_loss(decoded_data, torch.argmax(kb_one_hot_data, 1))\n",
    "            self.E_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.E_optimizer.step()\n",
    "            avg_loss+=loss\n",
    "        print('E_Epoch:{}, ELoss:{:.4f}'.format(e+1,float(avg_loss/iters)))\n",
    "        if (avg_loss/iters) < 0.1:\n",
    "            torch.save(self.encoder.mapper.state_dict(), f'alternate_trained_models/pluto_alternate_AE_encoder_model_k{self.k}N{self.n}c{count}.pth')\n",
    "\n",
    "    \n",
    "    def decoder_training(self, count):\n",
    "\n",
    "        self.demapper.train()\n",
    "        avg_loss=0.0\n",
    "        iters=100\n",
    "        for e in range(iters):\n",
    "            kb_one_hot_data = self.data_generate()\n",
    "            # print(kb_one_hot_data)\n",
    "            encoded_data_bits = self.encoder(kb_one_hot_data)\n",
    "            # print(encoded_data_bits)\n",
    "            received_data_symbols = torch.zeros((self.batch_size, self.n))\n",
    "\n",
    "            for pt,i in zip(encoded_data_bits.detach().numpy(), range(self.batch_size)):\n",
    "                \n",
    "                self.pluto_tx.Modulation(bits=pt, sps=self.sps, repeat_tx=True, plot=False)\n",
    "\n",
    "                received_symbols=self.pluto_rx.demodulate(plot=False)\n",
    "                received_data_symbols[i] = torch.from_numpy(received_symbols.astype('float64'))\n",
    "                self.pluto_tx.sdr.tx_destroy_buffer()\n",
    "\n",
    "            \n",
    "            # print(received_data_symbols)\n",
    "            received_data_symbols.requires_grad_(True)\n",
    "            decoded_data = self.demapper(received_data_symbols)\n",
    "            loss = self.D_loss(decoded_data, torch.argmax(kb_one_hot_data, 1))\n",
    "            self.D_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.D_optimizer.step()\n",
    "            avg_loss+=loss\n",
    "\n",
    "        print('DEpoch:{}, DLoss:{:.4f}'.format(e+1,float(avg_loss/iters)))\n",
    "        if (avg_loss/iters) < 0.1:\n",
    "            torch.save(self.demapper.s2b_mapper.state_dict(), f'alternate_trained_models/pluto_alternate_AE_decoder_model_k{self.k}N{self.n}c{count}.pth')\n",
    "\n",
    "\n",
    "    def evaluator(self):\n",
    "        \n",
    "        self.encoder.mapper.load_state_dict(torch.load('alternate_trained_models/pluto_alternate_AE_encoder_model_k4N7c35.pth'))\n",
    "        self.demapper.s2b_mapper.load_state_dict(torch.load('alternate_trained_models/pluto_alternate_AE_decoder_model_k4N7c36.pth'))\n",
    "\n",
    "        self.encoder.eval()\n",
    "        self.demapper.eval()\n",
    "\n",
    "        acc = 0.0\n",
    "        iters = 100\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for e in range(iters):\n",
    "                kb_one_hot_data = self.data_generate()\n",
    "                kb_one_hot_data.requires_grad_(False)\n",
    "\n",
    "                encoded_data_bits = self.encoder(kb_one_hot_data)\n",
    "                received_data_symbols = torch.zeros((self.batch_size, self.n))\n",
    "\n",
    "                for pt,i in zip(encoded_data_bits.detach().numpy(), range(self.batch_size)):\n",
    "                    \n",
    "                    self.pluto_tx.Modulation(bits=pt, sps=self.sps, repeat_tx=True, plot=False)\n",
    "\n",
    "                    received_symbols=self.pluto_rx.demodulate(plot=False)\n",
    "                    received_data_symbols[i] = torch.from_numpy(received_symbols.astype('float32'))\n",
    "                    self.pluto_tx.sdr.tx_destroy_buffer()\n",
    "                \n",
    "                decoded_data = self.demapper(received_data_symbols)\n",
    "                decoded_data = decoded_data.detach().numpy()\n",
    "                kb_one_hot_data= kb_one_hot_data.detach().numpy()\n",
    "\n",
    "                pred_output = np.argmax(decoded_data,1)\n",
    "                accuracy = np.equal(pred_output,np.argmax(kb_one_hot_data,1))\n",
    "                acc += np.mean(accuracy)\n",
    "\n",
    "            bler = 1-(acc/(iters))\n",
    "\n",
    "        return bler\n",
    "    \n",
    "    def trainer(self):\n",
    "\n",
    "        for i in range(120):\n",
    "            if i%2==1:\n",
    "                self.encoder_training(i)\n",
    "            else:\n",
    "                self.decoder_training(i)\n",
    "\n",
    "    def evaluation(self):\n",
    "        itrs = 40\n",
    "        self.blers = np.zeros((itrs,))\n",
    "        for i in range(itrs):\n",
    "            self.blers[i] = self.evaluator()\n",
    "        return np.mean(self.blers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLER of aletrnately trained NN: 0.27640000000000003\n"
     ]
    }
   ],
   "source": [
    "n=7\n",
    "k=4\n",
    "act_fn = nn.ReLU\n",
    "batch_size = 100\n",
    "\n",
    "tester = Communication_system_training(n=n, k=k, act_fn=act_fn, batch_size=batch_size)\n",
    "bler = tester.evaluation()\n",
    "print(f'BLER of aletrnately trained NN: {bler}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tester.encoder(torch.eye(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor(1),\n",
       "indices=tensor(2))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0,0,1,1,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0812, grad_fn=<KthvalueBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_values = tester.encoder(torch.eye(16))\n",
    "# for enc in enc_values[:4]:\n",
    "#     plt.plot(enc)\n",
    "a = torch.cdist(enc_values, enc_values).flatten()\n",
    "torch.kthvalue(a,17)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.32261172, 0.2103274 , 0.15564144, 0.20951006,\n",
       "        0.51217711, 0.38889956, 0.08116017, 0.15269378, 0.19730514,\n",
       "        0.38489628, 0.21250346, 0.93465376, 0.37566152, 0.86064446,\n",
       "        0.39739442],\n",
       "       [0.32261172, 0.        , 0.15922119, 0.25854859, 0.42362306,\n",
       "        0.51945448, 0.16667698, 0.38014689, 0.43767682, 0.46078432,\n",
       "        0.60021621, 0.1695472 , 0.95727563, 0.57905036, 0.70047504,\n",
       "        0.59046239],\n",
       "       [0.2103274 , 0.15922119, 0.        , 0.21014559, 0.26648897,\n",
       "        0.53388536, 0.18854664, 0.25471386, 0.32115415, 0.3228744 ,\n",
       "        0.46078759, 0.15021954, 0.9629758 , 0.42842516, 0.75405085,\n",
       "        0.47020191],\n",
       "       [0.15564144, 0.25854859, 0.21014559, 0.        , 0.32001677,\n",
       "        0.43411794, 0.35279268, 0.22227189, 0.28145236, 0.32701689,\n",
       "        0.49463555, 0.13945183, 0.91271269, 0.48042566, 0.83424306,\n",
       "        0.49403116],\n",
       "       [0.20951006, 0.42362306, 0.26648897, 0.32001677, 0.        ,\n",
       "        0.64335233, 0.41713852, 0.17088884, 0.2391499 , 0.14616106,\n",
       "        0.2647368 , 0.34583321, 1.03587008, 0.20401719, 0.92651159,\n",
       "        0.3575294 ],\n",
       "       [0.51217711, 0.51945448, 0.53388536, 0.43411794, 0.64335233,\n",
       "        0.        , 0.54971093, 0.57892054, 0.51535797, 0.65069896,\n",
       "        0.72114336, 0.40582687, 0.52923107, 0.73716265, 0.71917379,\n",
       "        0.56613255],\n",
       "       [0.38889956, 0.16667698, 0.18854664, 0.35279268, 0.41713852,\n",
       "        0.54971093, 0.        , 0.43568727, 0.47894865, 0.48515332,\n",
       "        0.57333624, 0.24121189, 0.95269239, 0.54096621, 0.64386469,\n",
       "        0.55314761],\n",
       "       [0.08116017, 0.38014689, 0.25471386, 0.22227189, 0.17088884,\n",
       "        0.57892054, 0.43568727, 0.        , 0.15356964, 0.12544487,\n",
       "        0.33499205, 0.28332362, 0.99077088, 0.32798526, 0.91810167,\n",
       "        0.39383888],\n",
       "       [0.15269378, 0.43767682, 0.32115415, 0.28145236, 0.2391499 ,\n",
       "        0.51535797, 0.47894865, 0.15356964, 0.        , 0.2012573 ,\n",
       "        0.33043119, 0.31702772, 0.87052613, 0.34630948, 0.87841839,\n",
       "        0.287826  ],\n",
       "       [0.19730514, 0.46078432, 0.3228744 , 0.32701689, 0.14616106,\n",
       "        0.65069896, 0.48515332, 0.12544487, 0.2012573 , 0.        ,\n",
       "        0.23010683, 0.37226263, 1.0415386 , 0.23694824, 0.96862507,\n",
       "        0.36245945],\n",
       "       [0.38489628, 0.60021621, 0.46078759, 0.49463555, 0.2647368 ,\n",
       "        0.72114336, 0.57333624, 0.33499205, 0.33043119, 0.23010683,\n",
       "        0.        , 0.5129894 , 1.05489194, 0.12391958, 1.00787711,\n",
       "        0.28890264],\n",
       "       [0.21250346, 0.1695472 , 0.15021954, 0.13945183, 0.34583321,\n",
       "        0.40582687, 0.24121189, 0.28332362, 0.31702772, 0.37226263,\n",
       "        0.5129894 , 0.        , 0.86051428, 0.500274  , 0.70411205,\n",
       "        0.47285962],\n",
       "       [0.93465376, 0.95727563, 0.9629758 , 0.91271269, 1.03587008,\n",
       "        0.52923107, 0.95269239, 0.99077088, 0.87052613, 1.0415386 ,\n",
       "        1.05489194, 0.86051428, 0.        , 1.08816969, 0.78371316,\n",
       "        0.80066216],\n",
       "       [0.37566152, 0.57905036, 0.42842516, 0.48042566, 0.20401719,\n",
       "        0.73716265, 0.54096621, 0.32798526, 0.34630948, 0.23694824,\n",
       "        0.12391958, 0.500274  , 1.08816969, 0.        , 1.009027  ,\n",
       "        0.32291383],\n",
       "       [0.86064446, 0.70047504, 0.75405085, 0.83424306, 0.92651159,\n",
       "        0.71917379, 0.64386469, 0.91810167, 0.87841839, 0.96862507,\n",
       "        1.00787711, 0.70411205, 0.78371316, 1.009027  , 0.        ,\n",
       "        0.82598758],\n",
       "       [0.39739442, 0.59046239, 0.47020191, 0.49403116, 0.3575294 ,\n",
       "        0.56613255, 0.55314761, 0.39383888, 0.287826  , 0.36245945,\n",
       "        0.28890264, 0.47285962, 0.80066216, 0.32291383, 0.82598758,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_values\n",
    "distance_matrix = np.zeros((16,16))\n",
    "for i,enc1 in zip(range(16),enc_values):\n",
    "    for j,enc2 in zip(range(16),enc_values):\n",
    "        distance_matrix[i,j] = np.linalg.norm(enc1-enc2)\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4561, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_dsit = torch.cdist(tester.encoder(torch.eye(16)), tester.encoder(torch.eye(16)))\n",
    "# np.array_equal(distance_matrix, py_dsit)\n",
    "py_dsit.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('alternately_trained_encodercodes_eye_16_input.npy', enc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.demapper(tester.encoder(torch.eye(16))).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(enc_values[2])\n",
    "plt.plot(enc_values[6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_enc = np.array([[0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 1., 0., 1.],\n",
    "       [0., 0., 1., 0., 0., 1., 1.],\n",
    "       [0., 0., 1., 1., 1., 1., 0.],\n",
    "       [0., 1., 0., 0., 1., 1., 0.],\n",
    "       [0., 1., 0., 1., 0., 1., 1.],\n",
    "       [0., 1., 1., 0., 1., 0., 1.],\n",
    "       [0., 1., 1., 1., 0., 0., 0.],\n",
    "       [1., 0., 0., 0., 1., 1., 1.],\n",
    "       [1., 0., 0., 1., 0., 1., 0.],\n",
    "       [1., 0., 1., 0., 1., 0., 0.],\n",
    "       [1., 0., 1., 1., 0., 0., 1.],\n",
    "       [1., 1., 0., 0., 0., 0., 1.],\n",
    "       [1., 1., 0., 1., 1., 0., 0.],\n",
    "       [1., 1., 1., 0., 0., 1., 0.],\n",
    "       [1., 1., 1., 1., 1., 1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.73205081, 1.73205081, 2.        , 1.73205081,\n",
       "        2.        , 2.        , 1.73205081, 2.        , 1.73205081,\n",
       "        1.73205081, 2.        , 1.73205081, 2.        , 2.        ,\n",
       "        2.64575131],\n",
       "       [1.73205081, 0.        , 2.        , 1.73205081, 2.        ,\n",
       "        1.73205081, 1.73205081, 2.        , 1.73205081, 2.        ,\n",
       "        2.        , 1.73205081, 2.        , 1.73205081, 2.64575131,\n",
       "        2.        ],\n",
       "       [1.73205081, 2.        , 0.        , 1.73205081, 2.        ,\n",
       "        1.73205081, 1.73205081, 2.        , 1.73205081, 2.        ,\n",
       "        2.        , 1.73205081, 2.        , 2.64575131, 1.73205081,\n",
       "        2.        ],\n",
       "       [2.        , 1.73205081, 1.73205081, 0.        , 1.73205081,\n",
       "        2.        , 2.        , 1.73205081, 2.        , 1.73205081,\n",
       "        1.73205081, 2.        , 2.64575131, 2.        , 2.        ,\n",
       "        1.73205081],\n",
       "       [1.73205081, 2.        , 2.        , 1.73205081, 0.        ,\n",
       "        1.73205081, 1.73205081, 2.        , 1.73205081, 2.        ,\n",
       "        2.        , 2.64575131, 2.        , 1.73205081, 1.73205081,\n",
       "        2.        ],\n",
       "       [2.        , 1.73205081, 1.73205081, 2.        , 1.73205081,\n",
       "        0.        , 2.        , 1.73205081, 2.        , 1.73205081,\n",
       "        2.64575131, 2.        , 1.73205081, 2.        , 2.        ,\n",
       "        1.73205081],\n",
       "       [2.        , 1.73205081, 1.73205081, 2.        , 1.73205081,\n",
       "        2.        , 0.        , 1.73205081, 2.        , 2.64575131,\n",
       "        1.73205081, 2.        , 1.73205081, 2.        , 2.        ,\n",
       "        1.73205081],\n",
       "       [1.73205081, 2.        , 2.        , 1.73205081, 2.        ,\n",
       "        1.73205081, 1.73205081, 0.        , 2.64575131, 2.        ,\n",
       "        2.        , 1.73205081, 2.        , 1.73205081, 1.73205081,\n",
       "        2.        ],\n",
       "       [2.        , 1.73205081, 1.73205081, 2.        , 1.73205081,\n",
       "        2.        , 2.        , 2.64575131, 0.        , 1.73205081,\n",
       "        1.73205081, 2.        , 1.73205081, 2.        , 2.        ,\n",
       "        1.73205081],\n",
       "       [1.73205081, 2.        , 2.        , 1.73205081, 2.        ,\n",
       "        1.73205081, 2.64575131, 2.        , 1.73205081, 0.        ,\n",
       "        2.        , 1.73205081, 2.        , 1.73205081, 1.73205081,\n",
       "        2.        ],\n",
       "       [1.73205081, 2.        , 2.        , 1.73205081, 2.        ,\n",
       "        2.64575131, 1.73205081, 2.        , 1.73205081, 2.        ,\n",
       "        0.        , 1.73205081, 2.        , 1.73205081, 1.73205081,\n",
       "        2.        ],\n",
       "       [2.        , 1.73205081, 1.73205081, 2.        , 2.64575131,\n",
       "        2.        , 2.        , 1.73205081, 2.        , 1.73205081,\n",
       "        1.73205081, 0.        , 1.73205081, 2.        , 2.        ,\n",
       "        1.73205081],\n",
       "       [1.73205081, 2.        , 2.        , 2.64575131, 2.        ,\n",
       "        1.73205081, 1.73205081, 2.        , 1.73205081, 2.        ,\n",
       "        2.        , 1.73205081, 0.        , 1.73205081, 1.73205081,\n",
       "        2.        ],\n",
       "       [2.        , 1.73205081, 2.64575131, 2.        , 1.73205081,\n",
       "        2.        , 2.        , 1.73205081, 2.        , 1.73205081,\n",
       "        1.73205081, 2.        , 1.73205081, 0.        , 2.        ,\n",
       "        1.73205081],\n",
       "       [2.        , 2.64575131, 1.73205081, 2.        , 1.73205081,\n",
       "        2.        , 2.        , 1.73205081, 2.        , 1.73205081,\n",
       "        1.73205081, 2.        , 1.73205081, 2.        , 0.        ,\n",
       "        1.73205081],\n",
       "       [2.64575131, 2.        , 2.        , 1.73205081, 2.        ,\n",
       "        1.73205081, 1.73205081, 2.        , 1.73205081, 2.        ,\n",
       "        2.        , 1.73205081, 2.        , 1.73205081, 1.73205081,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix = np.zeros((16,16))\n",
    "for i,enc1 in zip(range(16),hamming_enc):\n",
    "    for j,enc2 in zip(range(16),hamming_enc):\n",
    "        distance_matrix[i,j] = np.linalg.norm(enc1-enc2)\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39;49mplot(enc_values[\u001b[39m2\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m plt\u001b[39m.\u001b[39mplot(hamming_enc[\u001b[39m2\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Desktop/Casual Learning/casual/lib/python3.9/site-packages/matplotlib/pyplot.py:2748\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2747\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2748\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2749\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2750\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Casual Learning/casual/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1670\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1668\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1669\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m-> 1670\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_line(line)\n\u001b[1;32m   1671\u001b[0m \u001b[39mif\u001b[39;00m scalex:\n\u001b[1;32m   1672\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request_autoscale_view(\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Casual Learning/casual/lib/python3.9/site-packages/matplotlib/axes/_base.py:2333\u001b[0m, in \u001b[0;36m_AxesBase.add_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   2330\u001b[0m \u001b[39mif\u001b[39;00m line\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2331\u001b[0m     line\u001b[39m.\u001b[39mset_clip_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch)\n\u001b[0;32m-> 2333\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_line_limits(line)\n\u001b[1;32m   2334\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mget_label():\n\u001b[1;32m   2335\u001b[0m     line\u001b[39m.\u001b[39mset_label(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_child\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Casual Learning/casual/lib/python3.9/site-packages/matplotlib/axes/_base.py:2356\u001b[0m, in \u001b[0;36m_AxesBase._update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   2352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_line_limits\u001b[39m(\u001b[39mself\u001b[39m, line):\n\u001b[1;32m   2353\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m \u001b[39m    Figures out the data limit of the given line, updating self.dataLim.\u001b[39;00m\n\u001b[1;32m   2355\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2356\u001b[0m     path \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39;49mget_path()\n\u001b[1;32m   2357\u001b[0m     \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mvertices\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2358\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Casual Learning/casual/lib/python3.9/site-packages/matplotlib/lines.py:1031\u001b[0m, in \u001b[0;36mLine2D.get_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invalidy \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invalidx:\n\u001b[0;32m-> 1031\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecache()\n\u001b[1;32m   1032\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path\n",
      "File \u001b[0;32m~/Desktop/Casual Learning/casual/lib/python3.9/site-packages/matplotlib/lines.py:664\u001b[0m, in \u001b[0;36mLine2D.recache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39mif\u001b[39;00m always \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invalidy:\n\u001b[1;32m    663\u001b[0m     yconv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_yunits(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_yorig)\n\u001b[0;32m--> 664\u001b[0m     y \u001b[39m=\u001b[39m _to_unmasked_float_array(yconv)\u001b[39m.\u001b[39mravel()\n\u001b[1;32m    665\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y\n",
      "File \u001b[0;32m~/Desktop/Casual Learning/casual/lib/python3.9/site-packages/matplotlib/cbook/__init__.py:1369\u001b[0m, in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mma\u001b[39m.\u001b[39masarray(x, \u001b[39mfloat\u001b[39m)\u001b[39m.\u001b[39mfilled(np\u001b[39m.\u001b[39mnan)\n\u001b[1;32m   1368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1369\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(x, \u001b[39mfloat\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/Casual Learning/casual/lib/python3.9/site-packages/torch/_tensor.py:958\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(enc_values[2])\n",
    "plt.plot(hamming_enc[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1aac455f0641f991653f6a5b41b0d141343255f97d8ebe6117f7dfd8e782eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
